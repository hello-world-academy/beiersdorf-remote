{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SOLUTION - Polynomial Regression: The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main objective\n",
    "> __Fit a polynomial regression model to the given data. Consider the order of the polynomial as a hyperparameter and find its best value by applying *grid search*.__  \n",
    "\n",
    "The data file is named `example_data.csv` and it is found in the `../data/` folder. The response variable is denoted as `y` and the explanatory variable as `x`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested workflow\n",
    "\n",
    "* Load the relevant Python modules and libraries.\n",
    "* Load the data set and inspect the data by plotting it.\n",
    "* Split the data into a training and a test set.\n",
    "* Define a reasonable model metric (e.g. root mean square error).\n",
    "* Model building: Build 6 different polynomial regression models, with degrees of $k = 1,2,3,5,9,14$. \n",
    "* For each model calculate the model metric on the training set and on the validation.\n",
    "* Plot the data together with the regression line, given by each particular model. \n",
    "* Finally report the best `k` with respect to the model metric evaluated on the test set.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data = pd.read_csv(\"../data/example_data.csv\", index_col=0)\n",
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data.plot.scatter(x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X = data.x.values.reshape(-1, 1)\n",
    "y = data.y.values.reshape(-1, 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.scatter(x=X_train, y=y_train,\n",
    "           alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "ax.scatter(x=X_val, y=y_val,\n",
    "           alpha=0.5, s=60, color=\"g\", label=\"Validation set\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building\n",
    "\n",
    "The Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) is a special type of linear regression in which the relationship between the predictor variable $x$ and the response variable $y$ is modeled by a k<sup>th</sup>-degree polynomial in $x$. The incorporation of k<sup>th</sup>-degree polynomials results in a nonlinear relation between $y$ and $x$, but between the parameters $(\\beta_i)$ and the expected observations is linear. The model equation can be written as \n",
    "\n",
    "$$\\hat y = \\beta_0+\\beta_1x+\\beta_2x^2+...+\\beta_kx^k+\\epsilon$$\n",
    "\n",
    "Finding the optimal parameter combination is done by minimizing the **sum of squared errors (SSE)**, given by the equation\n",
    "\n",
    "$$SSE = \\sum e^2 = \\sum (\\hat y - y)^2 $$\n",
    "\n",
    "By fitting a polynomial to observations there arises the problem of choosing the order $k$ of the polynomial. How to choose the right number for the polynomial is a matter of an important concept called **model comparison** or [**model selection**](https://en.wikipedia.org/wiki/Model_selection). To keep it simple we use the [**root-mean-square error  (RMSE)**](https://en.wikipedia.org/wiki/Root-mean-square_deviation) defined by\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat y - y)^2}{n}}$$\n",
    "\n",
    "to evaluate the goodness-of-fit of the model. \n",
    "\n",
    "The `scikit-learn` library provides many model metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model metric (e.g. root mean square error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    rmse = np.sqrt(mean_absolute_error(y_true, y_pred))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamter: Generate polynomial and interaction features (Feature engineering).\n",
    "\n",
    "\n",
    "$$\\text{e.g. 2nd order:} \\qquad (x,y) \\to (x,y,x^2, xy,y^2)$$ \n",
    "\n",
    "\n",
    "\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` library provides powerful functionality to create polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build 6 different polynomial regression models, with degrees of $k = 1,2,3,5,9,14$.\n",
    "\n",
    "_Hint: Start building one model and then expand your approach_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model with $k=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the model metric on the training set and on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "pred_y_train = model.predict(y_train)\n",
    "pred_y_val = model.predict(y_val)\n",
    "print(\"RMSE on training data: \", rmse(y_train, pred_y_train))\n",
    "print(\"RMSE on validation data: \", rmse(y_val, pred_y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model with $k=2$ and evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train2 = poly.fit_transform(X_train)\n",
    "X_val2 = poly.fit_transform(X_val)\n",
    "\n",
    "#y_train2 = poly.fit_transform(y_train)\n",
    "#y_val2 = poly.fit_transform(y_val)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "pred_y_train2 = model.predict(X_train2)\n",
    "pred_y_val2 = model.predict(X_val2)\n",
    "print(\"RMSE on training data: \", rmse(y_train, pred_y_train2))\n",
    "print(\"RMSE on validation data: \", rmse(y_val, pred_y_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data together with the regression line, given by each particular model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_predict_reg_line = poly.transform(np.linspace(0,1, 100).reshape(-1,1))\n",
    "y_reg_line = model.predict(X_predict_reg_line)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(np.linspace(0,1, 100), y_reg_line, label=\"Regression line\")\n",
    "ax.scatter(x=X_train, y=y_train,\n",
    "           alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "ax.scatter(x=X_val, y=y_val,\n",
    "           alpha=0.5, s=60, color=\"g\", label=\"Validation set\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the purpose of reproducibility we write a function to build polynomial regression models of any order and evaluates them on a series of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial_model(X_train, y_train, X_val, y_val, degree, X_predict=np.linspace(0,1, 100)):\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    ### create polynomial features ###\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    poly.fit(X_train)\n",
    "    # polynomials for the train set\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    # polynomials for the validation set\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    # polynomials for the regression line\n",
    "    X_predict_reg_line = poly.transform(X_predict.reshape(-1,1))\n",
    "        \n",
    "    ### create the model ###\n",
    "    model = linear_model.LinearRegression()\n",
    "    \n",
    "    ### fit the model ###\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "    ### predict on training set ###\n",
    "    y_train_predict = model.predict(X_train_poly)\n",
    "    ## RMSE on training set\n",
    "    rmse_train = rmse(y_train_predict, y_train)\n",
    "\n",
    "    ### predict validation set ###\n",
    "    y_val_predict = model.predict(X_val_poly)\n",
    "    ## RMSE on validation set\n",
    "    rmse_val = rmse(y_val_predict, y_val)\n",
    "    \n",
    "    ### create regression line ###\n",
    "    y_predict_reg_line = model.predict(X_predict_reg_line)\n",
    "\n",
    "    return rmse_train, rmse_val, y_predict_reg_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rmse_train, \n",
    " rmse_val, \n",
    " y_predict_reg_line) = build_polynomial_model(X_train,\n",
    "                                              y_train, \n",
    "                                              X_val,\n",
    "                                              y_val,\n",
    "                                              degree=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_polynomial_model(X_train,\n",
    "                               y_train,\n",
    "                               X_val,\n",
    "                               y_val,\n",
    "                               X_predict=np.linspace(0,1, 100),\n",
    "                               degree=5, ax=None):\n",
    "    (rmse_train, \n",
    "     rmse_val, \n",
    "     y_predict_reg_line) = build_polynomial_model(X_train,\n",
    "                                              y_train, \n",
    "                                              X_val,\n",
    "                                              y_val,\n",
    "                                              degree=degree, \n",
    "                                              X_predict=X_predict)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "        \n",
    "    ax.scatter(x=X_train, y=y_train,\n",
    "           alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "    ax.scatter(x=X_val, y=y_val,\n",
    "           alpha=0.5, s=60, color=\"g\", label=\"Validation set\")\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.plot(X_predict, y_predict_reg_line)\n",
    "    ax.text(s=r\"$RMSE_T:$ {}\".format(np.round(rmse_train,3)), \n",
    "            x=0.75, y=1.15, size=16)\n",
    "    ax.text(s=r\"$RMSE_V:$ {}\".format(np.round(rmse_val,3)), \n",
    "            x=0.75,y=0.85, size=16)\n",
    "    ax.set_ylim(-1.95,1.95)\n",
    "    ax.set_title(\"Polynomial Regression (degree: {})\".format(degree), \n",
    "                 size=18);    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize the results for different__  $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(8,4))\n",
    "visualize_polynomial_model(X_train=X_train, y_train=y_train,\n",
    "                           X_val=X_val, y_val=y_val,\n",
    "                           degree=6, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the best `k` with respect to the model metric evaluated on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(4,2,figsize=(15,16))\n",
    "axes = axes.ravel()\n",
    "rmse_train_metric = []\n",
    "rmse_val_metric = []\n",
    "degrees = [1,2,3,5,7,9,10,12]\n",
    "for e, degree in enumerate(degrees):\n",
    "    \n",
    "    (rmse_train, \n",
    "     rmse_val, \n",
    "     _) = build_polynomial_model(X_train,\n",
    "                                 y_train,\n",
    "                                 X_val,\n",
    "                                 y_val,\n",
    "                                 degree=degree)\n",
    "    \n",
    "    \n",
    "    visualize_polynomial_model(X_train=X_train, y_train=y_train,\n",
    "                           X_val=X_val, y_val=y_val,\n",
    "                           degree=degree, ax=axes[e])\n",
    "    \n",
    "    rmse_train_metric.append(rmse_train)\n",
    "    rmse_val_metric.append(rmse_val)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, pretty plots! The figure shows, that if we increase $k$, the order of the polynomial, the curve becomes more flexible and it fits the data better and better. The better the data is fitted the lower becomes the error, RMSE. \n",
    "\n",
    "What is the best polynomial to fit the data?   \n",
    "\n",
    "Recall the goal is to learn the parameters from the data, thus we are interested in achieving a good generalization of the model and not necessarily perfectly fitted observation data. Such a behavior is known as [**overfitting**](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "\n",
    "For convenience we plot the RMSE against $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data frame\n",
    "res = pd.DataFrame({\"RMSE Training\":rmse_train_metric,\n",
    "                    \"RMSE Validation\":rmse_val_metric},\n",
    "                    index=degrees)\n",
    "# compute sweet spot\n",
    "sweet_spot_y = res[\"RMSE Validation\"].min()\n",
    "sweet_spot_x = res[\"RMSE Validation\"].idxmin()\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "res.plot(ax=ax, linewidth=4)\n",
    "ax.scatter(x=sweet_spot_x, y=sweet_spot_y, s=200, \n",
    "           alpha=0.5, label=\"Sweet spot\", color=\"g\")\n",
    "ax.plot((sweet_spot_x, sweet_spot_x),(0,sweet_spot_y),\n",
    "        linestyle=\"dashed\", color=\"g\")\n",
    "ax.plot((sweet_spot_x, sweet_spot_y),(sweet_spot_y, sweet_spot_y),\n",
    "        linestyle=\"dashed\", color=\"g\")\n",
    "\n",
    "ax.set_ylim(0.5,0.8)\n",
    "ax.set_xlim(0.5,12)\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Variance-Bias Tradeoff\", size=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that the error on the training data (blue line) is constantly decreasing. If we take a look at the RMSE for the validation set (orange line), we see that with increasing $k$, and thus increasing model complexity, the error decreases.\n",
    "\n",
    "Note that there is a sweet spot, indicated by the lowest RMSE on the validation set, where the model is just complex enough to generalize well on the so far unseen validation data. In our example the sweet spot is obtained for a regression model of 5<sup>th</sup> order. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
