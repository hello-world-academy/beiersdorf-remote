{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Beiersdorf spreadsheet challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/fake_beiersdorf_data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Understand your data! Conduct an exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/eda_fake_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: From one to many\n",
    "\n",
    "The goal is to \n",
    "1. read in a large dataset that stems from different Beiersdorf headquaters,\n",
    "2. subset the dataset based on the location of the headquater, and\n",
    "3. save that subset to disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Split large dataset into many small ones based on the variable `Name`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Use the `loc` to subset the dataset.    \n",
    "*Note: Uncomment the last few lines to actually save the resulting subsets of data to disk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data.Name.unique():\n",
    "    print(f'Computing {name} ...')\n",
    "    \n",
    "    subset = None # replace with your code\n",
    "    \n",
    "    #fname = f'subset_{name.replace(\" \", \"_\")}.csv'\n",
    "    #filepath = os.path.join('..', 'data', 'interim', fname)\n",
    "    #subset.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/subset_on_name.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: From many to one\n",
    "The goal is to load several spreadsheest from disk and combine them by adding one to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/interim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filepath = os.path.join('..', 'data', 'interim', '*.csv')\n",
    "filelist = glob.glob(filepath)\n",
    "filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Read the csv files from disk and combine them into one dataframe.  \n",
    "*Note: Use the a list and populate it with the dataframes, thereafter combine the dataframes by applying the `concat` function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in filelist:\n",
    "    # read the data\n",
    "    df = None   # your code here...\n",
    "    # append dataframes to a list\n",
    "    dfs           # your code here... \n",
    "# concatenate the dataframes\n",
    "fulldata = None   # your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/combine_csv_files.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Task 3: Restructure dataset to gain analytical insights\n",
    "\n",
    "The goal is to analyse the variable `Success` over time. **Is there a difference in that variable between the different headquaters of Beiersdorf?** Plot a chart that gives you insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.loc[data.Name == \"Beiersdorf AG\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Write a reusable function denoted as `transform_data` that transforms a dataset from a headquater into a dataframe with the date as its index and a column with booleans that refer to the variable `Success`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"Function to transform the Beiersdorf fake dataset\"\"\"\n",
    "    ## your code here...\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/transform_data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = transform_data(dataset)\n",
    "data_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Write an one-liner to plot the resulting dataframe `data_transformed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/one-liner-polt.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Write an end-to-end analysis\n",
    "> 1. Loop over all csv files from the individual headquaters\n",
    "2. Read the data using the `pd.read_csv` function\n",
    "3. Apply the `transform_data` function as defined above\n",
    "4. Append the dataframe to a list, named `dfs`\n",
    "5. Combine the dataframes in the list by applying the `pd.concat` function\n",
    "6. Assign the resulting full dataframe to a variable called `fulldata_transformed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/loop_and_transform_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/nicer_plot.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1f79ed0c15c2b10533bb75cd36d1971db8fc099917bb365d7c98cbcc4853389"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
